{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623299a3-a268-427b-9c0b-c7966f22524d",
   "metadata": {},
   "source": [
    "Q1. To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let's denote:\n",
    "\n",
    "A: Employee is a smoker\n",
    "B: Employee uses the health insurance plan\n",
    "We are given:\n",
    "\n",
    "P(B) = 0.7 (probability of an employee using the health insurance plan)\n",
    "P(A|B) = 0.4 (probability of an employee being a smoker given that they use the health insurance plan)\n",
    "We want to find P(A|B), the probability of an employee being a smoker given that they use the health insurance plan. Using Bayes' theorem:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P(B|A) is not directly given, but we can calculate it using the fact that:\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "We can calculate P(B|A) as follows:\n",
    "P(B|A) = (0.4 * 0.7) / P(A)\n",
    "\n",
    "Since we know that P(A) = P(B|A) + P(B|A') (where A' represents the complement of A), and P(B|A') = 1 - P(B|A), we can substitute the values to find P(A):\n",
    "\n",
    "P(A) = (0.4 * 0.7) / P(A) + (0.3 * 0.7)\n",
    "P(A) = 0.28 / P(A) + 0.21\n",
    "\n",
    "Simplifying, we have:\n",
    "P(A) - 0.21 = 0.28 / P(A)\n",
    "\n",
    "Multiplying both sides by P(A):\n",
    "P(A)^2 - 0.21 * P(A) = 0.28\n",
    "\n",
    "Rearranging the equation:\n",
    "P(A)^2 - 0.21 * P(A) - 0.28 = 0\n",
    "\n",
    "Solving this quadratic equation, we find two possible values for P(A). Taking the positive value since it represents a probability, we get:\n",
    "P(A) = 0.6078\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is approximately 0.6078 or 60.78%.\n",
    "\n",
    "Q2. The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions they make about the feature distributions:\n",
    "\n",
    "Bernoulli Naive Bayes assumes that features are binary or categorical, representing the presence or absence of certain characteristics. It models the occurrence (or absence) of each feature independently and uses binary values (0 or 1) to represent feature presence or absence.\n",
    "\n",
    "Multinomial Naive Bayes assumes that features follow a multinomial distribution, which means they represent counts or frequencies of discrete events. It is commonly used for text classification, where features often represent word frequencies or occurrences.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary/categorical features, while Multinomial Naive Bayes is suitable for discrete/count-based features.\n",
    "\n",
    "Q3. Bernoulli Naive Bayes handles missing values by considering them as a separate category or state of the feature. Instead of discarding instances with missing values, the classifier incorporates the information about missingness as an additional feature value. During training, the presence or absence of a feature is considered, and missing values are treated as a specific category. When making predictions, the classifier can handle instances with missing values by assigning probabilities based on the available information from other features.\n",
    "\n",
    "Q4. Gaussian Naive Bayes can be used for multi-class classification. It assumes that the continuous features follow a Gaussian (normal) distribution within each class. Each feature is modeled independently, and the class probability is calculated based on the product of the individual feature probabilities. By comparing the probabilities for each class, Gaussian Naive Bayes can determine the most likely class for a given instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4667189-ea73-42ac-9016-cde76faa3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the Spambase dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58801c9e-3e19-4f17-b4b5-812c326cdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset into features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]  \n",
    "y = data.iloc[:, -1]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce23e34-b1a0-4946-86c4-3fe4fc4840f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create instances of the Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d0d90b-dc15-443c-9a32-79ef2be64b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Perform 10-fold cross-validation\n",
    "cv = 10\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=cv)\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=cv)\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411635e-7343-443b-b711-b1a5565ebe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate performance metrics\n",
    "def print_metrics(scores):\n",
    "    accuracy = scores.mean()\n",
    "    precision = precision_score(y, bernoulli_nb.predict(X), average='macro')\n",
    "    recall = recall_score(y, bernoulli_nb.predict(X), average='macro')\n",
    "    f1 = f1_score(y, bernoulli_nb.predict(X), average='macro')\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1)\n",
    "\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print_metrics(bernoulli_scores)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print_metrics(multinomial_scores)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print_metrics(gaussian_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f07df-368c-4656-9527-4cef00bec484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f963f-24e0-4e41-bdd6-eff4b69979c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc81f3d-f9b2-4602-b02e-0f3141f063c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
