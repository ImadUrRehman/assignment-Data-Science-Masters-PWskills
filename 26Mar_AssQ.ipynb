{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690a19a8-c38e-4044-a9f0-d59eda8bae5c",
   "metadata": {},
   "source": [
    "Q1. Simple linear regression involves predicting a target variable using a single predictor variable, while multiple linear regression involves predicting a target variable using two or more predictor variables. For example, simple linear regression can be used to predict a person's weight based on their height, while multiple linear regression can be used to predict a person's weight based on their height, age, and gender.\n",
    "\n",
    "Q2. The assumptions of linear regression include linearity, independence, normality, equal variance, and absence of multicollinearity. To check these assumptions, one can use techniques such as scatter plots, residual plots, and Q-Q plots.\n",
    "\n",
    "Q3. The slope in a linear regression model represents the change in the target variable for every one-unit increase in the predictor variable, while the intercept represents the value of the target variable when the predictor variable is zero. For example, in a linear regression model predicting a person's weight based on their height, the slope would represent the increase in weight for every one-inch increase in height, while the intercept would represent the weight of a person who is zero inches tall (which is nonsensical in real life).\n",
    "\n",
    "Q4. Gradient descent is an iterative optimization algorithm used to minimize a cost function in machine learning. It works by adjusting the parameters of a model (such as the coefficients in a linear regression model) in small steps in the direction of steepest descent of the cost function. This process is repeated until convergence is achieved, i.e., the cost function reaches a minimum.\n",
    "\n",
    "Q5. The multiple linear regression model involves predicting a target variable using two or more predictor variables, while simple linear regression involves using a single predictor variable. The multiple linear regression model can capture more complex relationships between the predictor variables and the target variable.\n",
    "\n",
    "Q6. Multicollinearity in multiple linear regression refers to the presence of high correlation between two or more predictor variables. This can lead to unstable and unreliable coefficient estimates in the model. One can detect multicollinearity by calculating the correlation matrix between the predictor variables and looking for high correlations. To address this issue, one can consider removing one of the highly correlated predictor variables, or combining them into a single variable.\n",
    "\n",
    "Q7. The polynomial regression model is a type of regression model that allows for nonlinear relationships between the predictor variables and the target variable. It involves fitting a polynomial function to the data, rather than a straight line. This can be useful when the relationship between the variables is nonlinear.\n",
    "\n",
    "Q8. The advantage of polynomial regression over linear regression is that it can capture nonlinear relationships between the variables. However, the disadvantage is that it can be prone to overfitting, especially when using high-order polynomials. Polynomial regression may be preferred in situations where the relationship between the variables is nonlinear, but caution should be exercised to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
