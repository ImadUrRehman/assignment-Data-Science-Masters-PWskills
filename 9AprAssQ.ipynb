{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9229ab-958a-4f7c-acc3-cb9736551211",
   "metadata": {},
   "source": [
    "Q1. Bayes' theorem is a fundamental concept in probability theory and statistics that allows us to update the probability of an event based on new evidence or information. It provides a mathematical framework for reasoning about uncertainty. The theorem is named after Thomas Bayes, an 18th-century English statistician and philosopher.\n",
    "\n",
    "Q2. The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given event B.\n",
    "P(B|A) is the conditional probability of event B given event A.\n",
    "P(A) and P(B) are the probabilities of events A and B, respectively.\n",
    "Q3. Bayes' theorem is used in various fields, including statistics, machine learning, data analysis, and artificial intelligence. It has practical applications in areas such as:\n",
    "\n",
    "Medical diagnosis: Bayes' theorem can help calculate the probability of a disease given certain symptoms or test results.\n",
    "Spam filtering: It is used to determine the probability of an email being spam based on the presence of certain words or patterns.\n",
    "Document classification: Bayes' theorem can be used to categorize documents into different classes based on their content.\n",
    "Risk assessment: It can assist in assessing the likelihood of specific events or outcomes based on available information.\n",
    "Q4. Bayes' theorem is closely related to conditional probability. Conditional probability calculates the probability of an event A occurring given that event B has already occurred. Bayes' theorem provides a way to update the conditional probability based on new information. It allows us to reverse the conditioning by calculating the probability of event B given event A.\n",
    "\n",
    "Q5. When choosing a type of Naive Bayes classifier for a given problem, you typically consider the characteristics of the features and the assumptions made by each classifier. The three main types of Naive Bayes classifiers are:\n",
    "\n",
    "Gaussian Naive Bayes: It assumes that the features follow a Gaussian (normal) distribution. This classifier is suitable for continuous numerical features.\n",
    "\n",
    "Multinomial Naive Bayes: It assumes that the features are generated from a multinomial distribution. This classifier is commonly used for discrete features, such as word counts in text classification.\n",
    "\n",
    "Bernoulli Naive Bayes: It assumes that the features are binary (0 or 1). This classifier is suitable for binary features, such as presence or absence of certain characteristics.\n",
    "\n",
    "The choice depends on the nature of the features in your dataset. If the features are continuous and approximately follow a normal distribution, Gaussian Naive Bayes can be used. For discrete features or binary features, you can consider Multinomial or Bernoulli Naive Bayes, respectively.\n",
    "\n",
    "Q6. To predict the class of the new instance using Naive Bayes, we need to calculate the posterior probabilities for each class and choose the class with the highest probability. Assuming equal prior probabilities for each class (P(A) = P(B) = 0.5), we can apply the Naive Bayes algorithm:\n",
    "\n",
    "Step 1: Calculate the likelihoods for each feature value given each class:\n",
    "P(X1 = 3 | A) = 4/13\n",
    "P(X1 = 3 | B) = 1/10\n",
    "\n",
    "P(X2 = 4 | A) = 3/13\n",
    "P(X2 = 4 | B) = 3/10\n",
    "\n",
    "Step 2: Calculate the likelihoods for the new instance:\n",
    "P(X1 = 3, X2 = 4 | A) = P(X1 = 3 | A) * P(X2 = 4 | A) = (4/13) * (3/13) = 12/169\n",
    "P(X1 = 3, X2 = 4 | B) = P(X1 = 3 | B) * P(X2 = 4 | B) = (1/10) * (3/10) = 3/100\n",
    "\n",
    "Step 3: Calculate the posterior probabilities using Bayes' theorem:\n",
    "P(A | X1 = 3, X2 = 4) = (P(X1 = 3, X2 = 4 | A) * P(A)) / P(X1 = 3, X2 = 4)\n",
    "P(B | X1 = 3, X2 = 4) = (P(X1 = 3, X2 = 4 | B) * P(B)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "Since the denominator, P(X1 = 3, X2 = 4), is the same for both classes, we can compare the numerators:\n",
    "\n",
    "Numerator for A: (12/169) * 0.5 = 6/169\n",
    "Numerator for B: (3/100) * 0.5 = 3/200\n",
    "\n",
    "Comparing the numerators, we find that the numerator for class A is larger than that for class B. Therefore, Naive Bayes would predict the new instance to belong to class A.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e043f-9abc-44ae-9aa9-bf8417942fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
