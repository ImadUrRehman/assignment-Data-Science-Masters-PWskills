{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b2e9fe-58ea-4ab1-8751-e727e114907f",
   "metadata": {},
   "source": [
    "Q1. The relationship between polynomial functions and kernel functions in machine learning algorithms, particularly in Support Vector Machines (SVM), is that polynomial functions can be used as kernel functions to transform the data into a higher-dimensional feature space. Kernel functions enable SVM to learn non-linear decision boundaries without explicitly computing the transformed feature space.\n",
    "\n",
    "Polynomial kernel functions compute the inner product between the transformed feature vectors in the higher-dimensional space, which allows SVM to implicitly learn complex polynomial decision boundaries. The polynomial kernel function is defined as K(x, y) = (x^T y + c)^d, where d is the degree of the polynomial, c is an optional constant, and x and y are input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9031ee59-5950-4f5b-bf1b-e652fd6e88ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the dataset and split into training and testing sets\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming X contains the feature vectors and y contains the labels\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Preprocess the data (e.g., scale the features)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "##Q2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset and split into training and testing sets\n",
    "# Assuming X contains the feature vectors and y contains the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (e.g., scale the features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the testing data\n",
    "y_pred = svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71425f5f-3a0b-4fc5-a777-908f4af97503",
   "metadata": {},
   "source": [
    "Q3. In Support Vector Regression (SVR), increasing the value of epsilon does not directly affect the number of support vectors. The number of support vectors in SVR is determined by the complexity of the data and the chosen kernel function, rather than the value of epsilon.\n",
    "\n",
    "Epsilon is the epsilon-tube around the predicted regression function, within which no penalty is associated with errors. It determines the margin of tolerance for errors in SVR. A larger epsilon allows for a wider margin and permits more data points to fall within the margin, potentially leading to a larger number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2650d39-803b-47af-bfbc-bf96cd813b1f",
   "metadata": {},
   "source": [
    "Q4. The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can significantly impact the performance of the model. Here's an explanation of each parameter and examples of when you might want to increase or decrease its value:\n",
    "\n",
    "Kernel function: The choice of kernel function determines the mapping of the data into a higher-dimensional feature space. Different kernel functions have different characteristics and are suitable for different types of data. For example, the linear kernel is suitable for linearly separable data, while the RBF (Radial Basis Function) kernel is more flexible and can handle non-linear data.\n",
    "\n",
    "C parameter: The C parameter controls the trade-off between achieving a low training error and a low margin violation. A smaller C allows for a larger margin and more tolerance for errors, which can help reduce overfitting. On the other hand, a larger C puts more emphasis on achieving a smaller training error, potentially leading to a smaller margin and increased sensitivity to outliers.\n",
    "\n",
    "Epsilon parameter: The epsilon parameter defines the tube around the regression function within which no penalty is associated with errors. A larger epsilon allows for a wider margin of tolerance for errors, while a smaller epsilon leads to a smaller margin. The choice of epsilon depends on the acceptable level of error in the regression predictions.\n",
    "\n",
    "Gamma parameter: The gamma parameter defines the influence of each training sample in the SVR model. A smaller gamma implies a larger influence, causing the model to consider nearby points for prediction. Increasing gamma makes the model focus only on closer data points, leading to more complex decision boundaries. If the data is spread out, a larger gamma value might be suitable, whereas for data with clusters, a smaller gamma value might be preferred.\n",
    "\n",
    "The optimal values for these parameters depend on the specific dataset and problem at hand. It is common practice to perform hyperparameter tuning, such as using grid search or random search, to find the best combination of parameter values that optimize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3307a1b5-253e-41e0-8b0f-81cca1c725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Q5\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svm_tuned = SVC(**best_params)\n",
    "svm_tuned.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svm_tuned, 'svm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6269-9f3b-4122-ade2-34e29f20379e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
